---
title: 'Invisible AI and The Edges of The Unknown'
date: '2022-10-20'
lastmod: '2022-10-20'
draft: false
summary: 'This post invites readers into a thought-provoking journey about the perception and evolution of AI, contrasting its intangible nature with human form and exploring its impact as a media entity. It reflects on the rapid advancements in AI, using examples like Dall-E and StableDiffusion, and ponders the future of artificial general intelligence (AGI).'
tags:
  [
    'Artificial Intelligence',
    'Dall-E',
    'StableDiffusion',
    'Technology Evolution',
    'Human-AI Comparison',
    'Creative Process',
    'Future of AI',
    'Artificial General Intelligence',
    'from-substack',
  ]

images: ['/static/images/twitter-card.png']
---

Here's a thought experiment: when you think of AI, what's the first thing that comes to mind? (_feel free to reply/comment with your answer, but you don't have to. Keep reading_.)

Part of my creative and idea generation process is writing in the morning, and let that be an anchor to a practice that keeps moving throughout the day as ideas flow and grow. And so it becomes easier to catch myself thinking about something that's worth "thinking more seriously about later".

Over the past couple of weeks, and since I published [\[issue#2\]](https://zakelfassi.com/migrated-substack/changing-culture), one theme kept repeating. Daily.

AI.

Now I think the problem with experiencing AI is sort of like that of experiencing a Human being – how could we think of a "human being" as an abstract entity, without resorting to picturing a humanoid? One can't separate thinking about the human being without its flesh, similarly to how we didn't think much of AI before image generation models, until Dall-E made it become the new craze.

**Here's the TL;DR of what happened since I first showcased OpenAI's Dall-E in** [\[issue#2\]](https://zakelfassi.com/migrated-substack/changing-culture)**:** while Dall-E was still in private beta, a few similar tools started to popup, leveraging the market demand of Dall-E – they were in beta so you could see some images from some people they decided to give access to, but you couldn't play; but because humans like to play, StableDiffusion came (a story about money meets science: hedge fund guy with interests in AI meets scientists who were exploring doing more efficient AI but lacked funding) and not only released a similar technology to Dall-E's image generation publicly, but released it as an open source product. The ramifications of this is such that any good developer could build on top of the same foundation OpenAI's billion dollar fund was trying to get slowly rolled out to the public, without spending a dollar.

**All of a sudden, science didn't need money.** And so following StableDiffusion's public model release, everyone started doing science. And in a few weeks, we have gone **from** "_oh here's zak's newsletter with some AI generated images – I wonder if I can do that ... oh well, I'll input my email into Dall-E's waitlist while I watch all my friends post instagram stories about their own AI-generated Art_" **to** "_here's a video that I generated from a running a fork of StableDiffusion that can stitch multiple images together_".

**The future happened in a blink of an eye** – but not because that's how it happened. All these advances were decades in the making. But that's how it appeared it happened after the AI took its first visible form.

And similar to how we can't think of a human being without thinking of a humanoid, we've never thought of AI as anything – now we can think of it as media.

If this is the first time you are hearing about this, I can feel the shivers down your spine, because that's how I feel about what's next; if you're familiar with how this story is unfolding then, strap on, it gets better.

I think.

I hope.

This issue was long overdue – I just hope \[issue#4\] makes it before the AGI. Until then, stay safe, I guess?
